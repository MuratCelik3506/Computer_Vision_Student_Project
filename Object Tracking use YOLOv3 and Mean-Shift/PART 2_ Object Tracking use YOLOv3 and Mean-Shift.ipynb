{"cells":[{"cell_type":"markdown","metadata":{"id":"yZxzP3p0GnZo"},"source":["<h1 align=\"center\">Hacettepe University<br>Department of Computer Science</h1> \n","<h2 align=\"center\">BBM 418 - Computer Vision Laboratory<br>2022 Spring</h2>\n","<h3 align=\"center\">Assignment 4</h3>\n","<h3 align=\"left\">Name    :    Murat Çelik</h3>\n","<h3 align=\"left\">ID      :    21827263</h3>\n","<h4 align=\"right\">Due on May 25, 2022</h4>"]},{"cell_type":"markdown","metadata":{"id":"sOFFY_bcGscw"},"source":["## Table of Contents\n","- [1 - PART 2: Object Tracking use YOLOv3 and Mean-Shift](#1)\n","    - [1.1 - Required Libraries](#1.1)\n","    - [1.2 - Detect and Save data with YOLOv3](#1.2)\n","    - [1.3 - Filter the Main Person in Videos](#1.3)\n","    - [1.4 - Video Creation with Filtered Data](#1.4)\n","    - [1.5 - Video Creation with Filtered Data on Mean-Shift Algorithm](#1.5)\n","      - [1.5.1 - Mean-Shift Algorithm](#1.5.1)\n","    - [1.6 - Experimental Results](#1.6)\n","    - [1.7 - Conclusion](#1.7)\n","    - [1.8 - References](#1.8)"]},{"cell_type":"markdown","metadata":{"id":"870HkXnsGwgu"},"source":["# PART 2: Object Tracking use YOLOv3 and Mean-Shift <a name='1'></a>\n","- In this section, it is to detect the people in the video and estimate their location. This is called tracking by detection. For this, the YOLOv3 algorithm and the Mean Shift algorithm are used.\n","\n","- The YOLOv3 algorithm is tested on a few videos. And the data is obtained. Then, the Mean Shift algorithm is used in addition to the algorithm in the parts that YOLOv3 cannot detect and are missing. All these results are analyzed."]},{"cell_type":"markdown","metadata":{"id":"Iz1QLeyLI2-l"},"source":["## 1.1 Required Libraries <a name='1.1'></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36977,"status":"ok","timestamp":1653382402647,"user":{"displayName":"Murat Çelik","userId":"02057559685172804112"},"user_tz":-180},"id":"Efktxnc54M1k","outputId":"a14908fe-e66f-4425-ecc4-62e9e99073b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNjLuCbH4RG6"},"outputs":[],"source":["import os\n","try :\n","  os.chdir(\"drive/My Drive\")\n","  os.chdir(\"BBM418/Assignment 4\")\n","  os.chdir(\"Part 2\")\n","except: \n","  print(\"Already set\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BsgZIlP4RJi"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"FjczW7L7Y9A-"},"source":["## 1.2 Detect and Save data with YOLOv3 <a name='1.2'></a>\n","\n","YOLOv3 (You Only Look Once, Version 3) is a real-time object detection algorithm that identifies specific objects in videos, live feeds, or images. YOLO uses features learned by a deep convolutional neural network to detect an object. Versions 1-3 of YOLO were created by Joseph Redmon and Ali Farhadi."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7Ketbtw4UX2"},"outputs":[],"source":["def yolo_v3(img, net, classes, output_layers, height_origi, width_origi, colors = (0,255,0)):\n","    # prepare model \n","    height, width, channels = img.shape\n","    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","    net.setInput(blob)\n","    outs = net.forward(output_layers)\n","\n","    class_ids = []\n","    confidences = []\n","    boxes = []\n","    \n","    for out in outs:\n","        for detection in out:\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            if confidence > 0.5:\n","                # Object detected\n","                center_x = int(detection[0] * width)\n","                center_y = int(detection[1] * height)\n","                w = int(detection[2] * width)\n","                h = int(detection[3] * height)\n","                # Rectangle coordinates\n","                x = int(center_x - w / 2)\n","                y = int(center_y - h / 2)\n","                boxes.append([x, y, w, h]) # save boxes\n","                confidences.append(float(confidence))\n","                class_ids.append(class_id)\n","    # apply non maximum suppreison on predictions\n","    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","    font = cv2.FONT_HERSHEY_PLAIN\n","    boxes_file = list()\n","    for i in range(len(boxes)):\n","        if i in indexes:\n","            x, y, w, h = boxes[i]\n","            if class_ids[i] == 0:\n","              label = str(classes[class_ids[i]])\n","              x1,y1,w1,h1 = int(x/416*width_origi), int(y/416*height_origi), int(w/416*width_origi), int(h/416*height_origi)\n","              boxes_file.append([x1,y1,w1,h1]) # save data in original scale\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","    return img, boxes_file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPOiNBK04Ubt"},"outputs":[],"source":["def one_video(images, video_name, data_df, net, classes, output_layers):\n","  image_path = images[0]\n","  frame = cv2.imread(image_path)\n","  height, width, channels = frame.shape\n","  \n","  for count,image in enumerate(images,start=1):\n","      image_map = dict() # save all data as a dataFrame\n","      image_map[\"Video Name\"] = video_name\n","      image_map[\"Image Name\"] = image.split(\"/\")[-1]\n","      image_map[\"Frame\"] = count\n","\n","      frame = cv2.imread(image)\n","      height, width, channels = frame.shape\n","      frame = cv2.resize(frame, (416, 416))\n","      # run yolo function and get predictions\n","      frame,boxes = yolo_v3(frame, net, classes, output_layers, height, width)\n","\n","      for box in boxes:\n","        image_map[\"X\"] = box[0]\n","        image_map[\"Y\"] = box[1]\n","        image_map[\"W\"] = box[2]\n","        image_map[\"H\"] = box[3]\n","        data_df = data_df.append(image_map, ignore_index=True)\n","      frame = cv2.resize(frame, (width, height))\n","\n","      if (cv2.waitKey(1) & 0xFF) == ord('q'): # Hit `q` to exit\n","          break\n","  cv2.destroyAllWindows()\n","  return data_df"]},{"cell_type":"markdown","metadata":{"id":"a0RXYvnLJtYt"},"source":["The paths of the images to be processed are recorded in the list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRkFRpFg4UVU"},"outputs":[],"source":["DATA_PATH = 'content/vot2017/'\n","person_track_subfolder = [\"gymnastics1\", \"iceskater1\", \"pedestrian1\"]\n","data_path = dict()\n","for subfolder in person_track_subfolder:\n","  path = DATA_PATH + subfolder + \"/color\"\n","  path_image_list = list()\n","  for insubfolder in sorted(os.listdir(path)):\n","    path_image_list.append(path + \"/\" + insubfolder)\n","  \n","  data_path[subfolder] = path_image_list"]},{"cell_type":"markdown","metadata":{"id":"wVQxktq4KDdt"},"source":["The YOLOv3 model is created in the code below. All people in each frame are detected. These detected data are saved in the file for each label with the name \"< label >.csv\". All data can be found in the folder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FmJFP4p0MvU0"},"outputs":[],"source":["for label in person_track_subfolder:\n","  net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\") # create model with weights\n","  classes = [\"person\"]\n","  layer_names = net.getLayerNames()\n","  output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","  df = pd.DataFrame(columns=[\"Video Name\", \"Image Name\", \"Frame\", \"X\", \"Y\", \"W\", \"H\"])\n","  df = one_video(data_path[label], label, df, net, classes, output_layers)\n","  df.to_csv(label + \"/\" + label + \".csv\")"]},{"cell_type":"markdown","metadata":{"id":"4uTJ6zfeZVOb"},"source":["## 1.3 Filter the Main Person in Videos <a name='1.3'></a>\n","The YOLOv3 algorithm detects every person in the videos. In this study, analyzes are made for a single person in the video. For this reason, YOLOv3's detections are filtered until only one box remains for each frame. This filtering process is done by calculating the IoU value of the ground truth data and the data of the YOLOv3 algorithm. If the obtained IoU score is above 40%, it is kept, otherwise it is deleted. Thus, the YOLOv3 algorithm produces results only for the object studied in the video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6BDYfECUX5y"},"outputs":[],"source":["def IoU(y_test, y_pred, threshold):\n","  x1,y1,w_test,h_test = y_test\n","  x2,y2 = x1 + w_test, y1 + h_test\n","  xx1, yy1, w_pred, h_pred = y_pred\n","  xx2,yy2 = xx1 + w_pred, yy1 + h_pred\n","  if intersect([x1,y1,x2,y2], [xx1,yy1,xx2,yy2]):\n","    return False, 0\n","  x_inter1 = max(x1,xx1)\n","  y_inter1 = max(y1,yy1)\n","  x_inter2 = min(x2,xx2)\n","  y_inter2 = min(y2,yy2)\n","  width_inter = abs(x_inter2 - x_inter1)\n","  height_inter = abs(y_inter2 - y_inter1)\n","  area_inter = height_inter * width_inter\n","  area_1 = abs(x2-x1) * abs(y2-y1)\n","  area_2 = abs(xx2-xx1) * abs(yy2-yy1)\n","  area_union = area_1 + area_2 - area_inter\n","  iou = area_inter / area_union\n","  return iou > threshold, iou\n","def intersect(y_test, y_pred):\n","  x1,y1,x2,y2 = y_test\n","  xx1,yy1,xx2,yy2 = y_pred\n","  if x2 < xx1 or xx2 < x1:\n","    return True\n","  if y2 < yy1 or yy2 < y1:\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwfs7CXExz-R"},"outputs":[],"source":["def filter_yolo(file_name):\n","  file = \"content/vot2017/\"+file_name+\"/groundtruth.txt\"\n","  f = open(file,\"r\")\n","  frame_gt = list()\n","  for x in f:\n","    gt_one_frame = list(map(int,list(map(float,x.split(\",\")))))\n","    x = gt_one_frame[0] \n","    y = gt_one_frame[1] \n","    w = abs(gt_one_frame[0] - gt_one_frame[4])\n","    h = abs(gt_one_frame[1] - gt_one_frame[5])\n","    frame_gt.append([x,y,w,h])\n","  f.close()\n","  df = pd.read_csv(file_name + \"/\" + file_name + \".csv\")\n","  df_filter = pd.DataFrame(columns = [\"Image Name\", \"X\", \"Y\", \"W\" ,\"H\"])\n","  for count,frame in enumerate(sorted(df[\"Image Name\"].value_counts().keys())):\n","    df_tmp = df[df[\"Image Name\"] == frame]\n","    y_test = frame_gt[count]\n","    for y in range(len(df_tmp)):\n","      tmp_row = df_tmp.iloc[y]\n","      x,y,w,h = tmp_row[\"X\"], tmp_row[\"Y\"], tmp_row[\"W\"], tmp_row[\"H\"],\n","      y_pred_tmp = [x,y,w,h]\n","      y_pred = y_pred_tmp.copy()\n","\n","      control, iou = IoU(y_test.copy(), y_pred, 0.4)\n","      if control:\n","        df_filter = df_filter.append({\"Image Name\" : frame, \"X\": x, \"Y\":y, \"W\":w, \"H\" : h}, ignore_index=True)\n","  df_filter.to_csv(label + \"/\" + label +\"_filter.csv\")\n","  return df_filter, frame_gt"]},{"cell_type":"markdown","metadata":{"id":"hOYIeCMzNZaO"},"source":[" ## 1.4 Video Creation with Filtered Data <a name='1.4'></a>\n","\n","In this section, there are two different data for each frame. One of these data is ground truth data. The other is the data that YOLOv3 predicts. It is known that ground truth data contains one data for each frame. Since YOLOv3's predictions have deficiencies and missed predictions, it cannot be said to contain one data for each frame.\n","\n","When creating the video, 2 colors are used for the boxes. *The **blue** ones of these boxes* show ground truth data. *The  **green** one of these boxes* shows the data of the YOLOv3 algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RCFccN1WInB"},"outputs":[],"source":["def write_video(data_path, frame_gt,video_name, df):\n","  colors = (0,255,0)\n","  images = data_path[video_name]\n","  image_path = images[0]\n","  frame = cv2.imread(image_path)\n","  height, width, channels = frame.shape\n","  fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n","  writer = cv2.VideoWriter(video_name + \"/\" + video_name +\"without_meanshift.mp4\", fourcc, 24, (width, height))\n","  for count,image in enumerate(images,start=1):\n","      frame = cv2.imread(image)\n","      height, width, channels = frame.shape\n","      # frame = cv2.resize(frame, (416, 416))\n","      df_filter_image = df[df[\"Image Name\"] == image.split(\"/\")[-1]]\n","      for boxes in range(len(df_filter_image)):\n","        box = df_filter_image.iloc[boxes]\n","        x,y,w,h = box[\"X\"], box[\"Y\"], box[\"W\"], box[\"H\"]\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), colors, 1)\n","      y_test = frame_gt[count-1]\n","      x,y,w,h = y_test[0], y_test[1],  y_test[2], y_test[3]\n","      cv2.rectangle(frame, (x, y), (x + w, y + h), (255,0,0), 1)\n","      # frame = cv2.resize(frame, (width, height))\n","      cv2_imshow(frame)\n","      writer.write(frame) # Write out frame to video\n","\n","      #v2_imshow(frame)\n","      if (cv2.waitKey(1) & 0xFF) == ord('q'): # Hit `q` to exit\n","          break\n","\n","  # Release everything if job is finished\n","  writer.release()\n","  cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APlEoGdzROcR"},"outputs":[],"source":["for label in person_track_subfolder:\n","  df_filter, frame_gt = filter_yolo(label)\n","  df_filter.to_csv(label + \"/\" + label+ \"_filter.csv\")\n","  write_video(data_path, frame_gt, label , df_filter)"]},{"cell_type":"markdown","metadata":{"id":"ivXLyAYpNo1m"},"source":[" ## 1.5 Video Creation with Filtered Data on Mean Shift Algorithm <a name='1.5'></a>\n","\n","In this section, the Mean Shift Algorithm is used. With this algorithm, the previous prediction data is analyzed for the frames where YOLOv3 is missing, and predictions are made. Thus, there is a chance to follow the tracked object for each frame.\n","\n"," When creating the video, 3 colors are used for the boxes. *The **blue** ones of these boxes* show ground truth data. T*he **green** one of these boxes* shows the data of the YOLOv3 algorithm. *The **red** one of these boxes* is the data of the **Mean-Shift algorithm**."]},{"cell_type":"markdown","metadata":{"id":"D2kwMaCjPFTO"},"source":[" ### 1.5.1 Mean-Shift Algorithm <a name='1.5.1'></a>\n"," The Mean Shift algorithm has a basic logic for object tracking. It makes inferences from previous predictions for unpredicted frames. The position of the object in the previous picture is given. High concentration of data points of that object are found. For the next frame, similar data points are scanned around that object and a new estimate is made."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gPrMc4UOpdl"},"outputs":[],"source":["def write_video_with_meanShift(data_path, frame_gt,video_name, df):\n","  # Store data as dataframe\n","  df_filter_meanShift = pd.DataFrame(columns = [\"Image Name\", \"X\", \"Y\", \"W\" ,\"H\"])\n","  images = data_path[video_name] \n","  image_path = images[0]\n","  frame = cv2.imread(image_path)\n","  height, width, channels = frame.shape\n","  # Video writer algorithm\n","  fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n","  writer = cv2.VideoWriter(video_name + \"/\" + video_name +\"_meanshift.mp4\", fourcc, 24, (width, height))\n","  x_last,y_last,w_last, h_last = 0,0,0,0 # The most recent estimate.\n","  previous_not_meanshift_control = True # Mean-Shift or YOLOV3 made the previous prediction.\n","  x,y,w,h = 0,0,0,0\n","  for count,image in enumerate(images,start=1):\n","      frame = cv2.imread(image)\n","      height, width, channels = frame.shape\n","      # frame = cv2.resize(frame, (416, 416))\n","      frame_name = image.split(\"/\")[-1]\n","      df_filter_image = df[df[\"Image Name\"] == frame_name]\n","      for boxes in range(len(df_filter_image)):\n","        box = df_filter_image.iloc[boxes]\n","        x,y,w,h = box[\"X\"], box[\"Y\"], box[\"W\"], box[\"H\"]\n","        df_filter_meanShift = df_filter_meanShift.append({\"Image Name\" : frame_name, \"X\": x, \"Y\":y, \"W\":w, \"H\" : h}, ignore_index=True)\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), (0,255,0), 1)\n","      if x == 0 and y==0 and w==0 and h==0:\n","        continue\n","      # The part where the Mean-Shift Algorithm is used.\n","      if len(df_filter_image) == 0:\n","        x, y, w, h = x_last,y_last,w_last, h_last\n","        track_window = (x, y, w, h)\n","        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n","        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n","        # apply meanshift to get the new location\n","        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n","        # Draw it on image\n","        x,y,w,h = track_window\n","        x_last,y_last,w_last, h_last = track_window\n","        df_filter_meanShift = df_filter_meanShift.append({\"Image Name\" : frame_name, \"X\": x, \"Y\":y, \"W\":w, \"H\" : h}, ignore_index=True)\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), (0,0,255), 1)\n","        previous_not_meanshift_control = False\n","      # Holds the last valid predictions for the Mean Shift algorithm.\n","      if(previous_not_meanshift_control):\n","        roi = frame[y:y+h, x:x+w]\n","        hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n","        mask = cv2.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\n","        roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])\n","        cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n","        term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n","      # Draws ground truth data.\n","      y_test = frame_gt[count-1]\n","      x_last,y_last,w_last, h_last = x,y,w,h\n","      x,y,w,h = y_test[0], y_test[1],  y_test[2], y_test[3]\n","      cv2.rectangle(frame, (x, y), (x + w, y + h), (255,0,0), 1)\n","      # frame = cv2.resize(frame, (width, height))\n","      cv2_imshow(frame)\n","      writer.write(frame) # Write out frame to video\n","      #v2_imshow(frame)\n","      if (cv2.waitKey(1) & 0xFF) == ord('q'): # Hit `q` to exit\n","          break\n","  writer.release()\n","  cv2.destroyAllWindows()\n","  return df_filter_meanShift"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pmXL05GdJgyAuniSoTrArHKQiWEl6khO"},"executionInfo":{"elapsed":106383,"status":"ok","timestamp":1653382605507,"user":{"displayName":"Murat Çelik","userId":"02057559685172804112"},"user_tz":-180},"id":"uXqogV2HOpgW","outputId":"54fbc9a0-f431-45f1-b768-e47d99beaaef"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["for label in [\"pedestrian1\"]:\n","  file = \"content/vot2017/\"+label+\"/groundtruth.txt\"\n","  f = open(file,\"r\")\n","  frame_gt = list()\n","  for x in f:\n","    gt_one_frame = list(map(int,list(map(float,x.split(\",\")))))\n","    x = gt_one_frame[0] \n","    y = gt_one_frame[1] \n","    w = abs(gt_one_frame[0] - gt_one_frame[4])\n","    h = abs(gt_one_frame[1] - gt_one_frame[5])\n","    frame_gt.append([x,y,w,h])\n","  f.close()\n","  df_filter = pd.read_csv(label + \"/\" + label+ \"_filter.csv\")\n","  df_filter_meanShift = write_video_with_meanShift(data_path, frame_gt, label, df_filter)\n","  df_filter_meanShift.to_csv(label + \"/\" + label+ \"_filter_withMeanShift.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ur6kv1B8VhlE"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e30HqKIe2qTE"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"hXLpfulZTBHc"},"source":[" ## 1.6 Experimental Results <a name='1.6'></a>\n","We have 2 types of estimation data. One of them contains only YOLOv3's predictions. The other includes predictions from YOLOv3 + Mean-Shift. These data are separately subjected to ground truth data and IoU (Intersection over Union) calculation. A score is kept for each frame. These scores are saved as files in the drive folder. The score of that frame is given as 0 for data with no conflicts or no predictions. All scores are summed and the last stage is divided by the number of frames.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssuv2V8f2sAJ"},"outputs":[],"source":["def IoU(y_test, y_pred):\n","  x1,y1,w_test,h_test = y_test\n","  x2,y2 = x1 + w_test, y1 + h_test\n","  xx1, yy1, w_pred, h_pred = y_pred\n","  xx2,yy2 = xx1 + w_pred, yy1 + h_pred\n","  if intersect([x1,y1,x2,y2], [xx1,yy1,xx2,yy2]):\n","    return 0\n","  x_inter1 = max(x1,xx1)\n","  y_inter1 = max(y1,yy1)\n","  x_inter2 = min(x2,xx2)\n","  y_inter2 = min(y2,yy2)\n","  width_inter = abs(x_inter2 - x_inter1)\n","  height_inter = abs(y_inter2 - y_inter1)\n","  area_inter = height_inter * width_inter\n","  area_1 = abs(x2-x1) * abs(y2-y1)\n","  area_2 = abs(xx2-xx1) * abs(yy2-yy1)\n","  area_union = area_1 + area_2 - area_inter\n","  iou = area_inter / area_union\n","  return iou\n","def intersect(y_test, y_pred):\n","  x1,y1,x2,y2 = y_test\n","  xx1,yy1,xx2,yy2 = y_pred\n","  if x2 < xx1 or xx2 < x1:\n","    return True\n","  if y2 < yy1 or yy2 < y1:\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AATsJ4R2xFE"},"outputs":[],"source":["def iou_calculate(label, meanshift):\n","  file = \"content/vot2017/\"+label+\"/groundtruth.txt\"\n","  f = open(file,\"r\")\n","  frame_gt = list()\n","  for x in f:\n","    gt_one_frame = list(map(int,list(map(float,x.split(\",\")))))\n","    x = gt_one_frame[0] \n","    y = gt_one_frame[1] \n","    w = abs(gt_one_frame[0] - gt_one_frame[4])\n","    h = abs(gt_one_frame[1] - gt_one_frame[5])\n","    frame_gt.append([x,y,w,h])\n","  f.close()\n","  if meanshift:\n","    df_filter = pd.read_csv(label + \"/\" + label+ \"_filter_withMeanShift.csv\")\n","  else:\n","    df_filter = pd.read_csv(label + \"/\" + label+ \"_filter.csv\")\n","  iou_list = dict()\n","  for count in range(len(frame_gt)):\n","    frame_num = count + 1\n","    tmp = len(str(frame_num))\n","    frame_num_name = str(frame_num)\n","    for _ in range(tmp,8):\n","      frame_num_name = \"0\" + frame_num_name \n","    frame_num_name = frame_num_name + \".jpg\"\n","    df_tmp = df_filter[df_filter[\"Image Name\"] == frame_num_name]\n","    if len(df_tmp) == 0:\n","      iou_list[frame_num_name] = 0\n","      continue\n","    total_iou = 0\n","    for index in range(len(df_tmp)):\n","      x,y,w,h = df_tmp.iloc[index][\"X\"], df_tmp.iloc[index][\"Y\"], df_tmp.iloc[index][\"W\"], df_tmp.iloc[index][\"H\"]\n","      iou = IoU(frame_gt[count], [x,y,w,h])\n","      total_iou += iou\n","    iou_list[frame_num_name] = total_iou / len(df_tmp)\n","  iou_series = pd.Series(iou_list)\n","  if meanshift:\n","    iou_series.to_csv(label + \"/\" + label + \"_iou_meanshift.csv\")\n","  else:\n","    iou_series.to_csv(label + \"/\" + label + \"_iou_without_meanshift.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLQglxEx24lm"},"outputs":[],"source":["for label in person_track_subfolder[::-1]:\n","  iou_calculate(label, True)\n","  iou_calculate(label, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbPCinlC26rk"},"outputs":[],"source":["person_track_subfolder = [\"gymnastics1\", \"iceskater1\", \"pedestrian1\"]\n","df_result = pd.DataFrame(index=[\"Mean Shift\", \"Non Mean Shift\"])\n","for label in person_track_subfolder[::-1]:\n","    iou_series_mean = pd.read_csv(label + \"/\" + label + \"_iou_meanshift.csv\")\n","    score_mean = iou_series_mean[\"0\"].sum() / len(iou_series_mean)\n","\n","    iou_series_notmean = pd.read_csv(label + \"/\" + label + \"_iou_without_meanshift.csv\")\n","    score_notmean = iou_series_notmean[\"0\"].sum() / len(iou_series_notmean)\n","\n","    df_result[label] = [score_mean, score_notmean]"]},{"cell_type":"markdown","metadata":{"id":"D7nNPoS8WtJA"},"source":["When we look at the scores, we can see that the Mean-Shift algorithm improves the results. It is predicted that the score may increase by making predictions on data sets that have never been predicted. It can be said that the algorithm makes good predictions at points of 2-3 frames, but the size of the error increases as the frame to be estimated increases.\n","\n","Except this:\n","\n","- It is observed that the ***best score*** is taken from the \"***pedestrian1***\" dataset. The reason why this score is the best can be said to be the main reason that the YOLOv3 algorithm makes more predictions in the data. Easy to distinguish the human object inside from the background, not including any zoom-in, zoom-out, convenience has been observed here.\n","\n","- It is observed that the ***average score*** is taken from the \"***gymnastics1***\" dataset. It can be said that the main reason for this score is the underestimation of the YOLOv3 algorithm in some parts of the data. There are various reasons for this situation. Difficulties have been observed here, as the human object in it cannot be easily distinguished from the background, and it contains zoom-in and zoom-out. There are also incomplete estimations due to the movements of the object in the data. The Mean-Shift algorithm contains incorrect predictions at these points.\n","\n","- It is observed that the ***worst score*** is taken from the \"***iceskater1***\" dataset. It can be said that the main reason for this score is the underestimation of the YOLOv3 algorithm in most parts of the data. The second main reason is that movements such as the rotation of the object in the video, shrinking and jumping cause difficulties in estimation. Difficulties have been observed here, as the human object in it cannot be easily distinguished from the background, it contains too much zoom-in and zoom-out. There have been points where the Mean-Shift algorithm has been overused, and these points contain erroneous predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1653366638691,"user":{"displayName":"Murat Çelik","userId":"02057559685172804112"},"user_tz":-180},"id":"nbo5AumR-XkU","outputId":"06021d78-4a32-4786-8092-f7aff339ddb9"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-4710276c-de10-421b-9701-f50b1dd5840e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pedestrian1</th>\n","      <th>iceskater1</th>\n","      <th>gymnastics1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Mean Shift</th>\n","      <td>0.589540</td>\n","      <td>0.351710</td>\n","      <td>0.399947</td>\n","    </tr>\n","    <tr>\n","      <th>Non Mean Shift</th>\n","      <td>0.583418</td>\n","      <td>0.291152</td>\n","      <td>0.317797</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4710276c-de10-421b-9701-f50b1dd5840e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4710276c-de10-421b-9701-f50b1dd5840e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4710276c-de10-421b-9701-f50b1dd5840e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                pedestrian1  iceskater1  gymnastics1\n","Mean Shift         0.589540    0.351710     0.399947\n","Non Mean Shift     0.583418    0.291152     0.317797"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#df_result.to_csv(\"accuracy.csv\")\n","df_result = pd.read_csv(\"accuracy.csv\")\n","df_result"]},{"cell_type":"markdown","metadata":{"id":"Hex_WCKfTQEs"},"source":["  ## 1.7 Conclusion <a name='1.7'></a>"]},{"cell_type":"markdown","metadata":{"id":"7c8SYddoValw"},"source":["In this part, we tested the object tracking algorithm. Videos were processed using YOLOv3 and Mean-Shift algorithms. All data is saved in the drive folder. Information was obtained about the difficulties of object tracking and the solution of these difficulties. Pytorch and Opencv libraries were used and experience was gained in computer vision."]},{"cell_type":"markdown","metadata":{"id":"yPLI6V7PTQJu"},"source":["## 1.8 References <a name='1.8'></a>\n","\n","[Mean-Shift Algorithm](https://docs.opencv.org/3.4/d7/d00/tutorial_meanshift.html)\n","\n","[YOLO for Object Detection, Architecture Explained!](https://medium.com/analytics-vidhya/understanding-yolo-and-implementing-yolov3-for-object-detection-5f1f748cc63a)\n","\n","[DataSet](https://www.votchallenge.net/vot2017/dataset.html)\n","\n","[YOLOv3: Real-Time Object Detection Algorithm (What’s New?)](https://viso.ai/deep-learning/yolov3-overview/)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Xm5B84AgTGL"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Iz1QLeyLI2-l","FjczW7L7Y9A-","4uTJ6zfeZVOb","hOYIeCMzNZaO","D2kwMaCjPFTO","hXLpfulZTBHc"],"machine_shape":"hm","name":"PART 2: Object Tracking use YOLOv3 and Mean-Shift.ipynb","provenance":[],"authorship_tag":"ABX9TyO0anHypKX9dNUMgNlI4Ou2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}